{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # download all NLTK modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tokenizing -\n",
    "        . Word Tokenizer (seperates by Words)\n",
    "        . Sentence Tokenizer (seperates by Sentences)\n",
    "    Corpora - \n",
    "          In linguistics, a corpus (plural corpora) or text corpus is a large and structured set of texts\n",
    "          these are around the same topics e.g : science journals, english Language.\n",
    "          A corpus may contain texts in a single language (monolingual corpus) or text data in multiple \n",
    "          languages (multilingual corpus).\n",
    "    Lexicons - \n",
    "          the vocabulary of a person, language, or branch of knowledge. kind of a dictonary\n",
    "          eg. investor speak 'bull' - someone positive about the market\n",
    "              general english 'bull' - the animal bull\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#import word and sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"Hello there, my name is abeeweeda. This is my github. Welcome here and find some dumb python code. How are you btw?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there, my name is abeeweeda.', 'This is my github.', 'Welcome here and find some dumb python code.', 'How are you btw?']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(sample_text))\n",
    "#sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', ',', 'my', 'name', 'is', 'abeeweeda', '.', 'This', 'is', 'my', 'github', '.', 'Welcome', 'here', 'and', 'find', 'some', 'dumb', 'python', 'code', '.', 'How', 'are', 'you', 'btw', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sample_text))\n",
    "#word tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Stop Words\n",
    "        . Though \"stop words\" usually refers to the most common words in a language, there is no single universal \n",
    "          list of stop words used by all natural language processing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence = \"this is to test how stop words work in nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "list(stop_words) # list of nltk english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'stop', 'words', 'work', 'nltk']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence)\n",
    "corpus_sentence = []\n",
    "for w in words:\n",
    "    if w not in list(stop_words):\n",
    "        corpus_sentence.append(w)\n",
    "\n",
    "corpus_sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'stop', 'words', 'work', 'nltk']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprehension\n",
    "courpus_sentence = [w for w in words if w not in list(stop_words)]\n",
    "corpus_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Stemming\n",
    "        . stemming is the process of reducing inflected (or sometimes derived) words to their word stem, \n",
    "          base or root form\n",
    "        . See below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #its as class so making object for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'python', 'python', 'pythonli', 'python']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythonly\", \"pythoned\"]\n",
    "stemmed_words = [ps.stem(w) for w in example_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "#state of union addresses by presidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Part of Speech Tagging\n",
    "        . Labeling each word to part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "#PunktSentenceTokenizer unisupervised ML Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "#loading data from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)#tagging part of speech to each word\n",
    "            print(tagged)\n",
    "    except Exeption as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    chunking\n",
    "        . Chunking is also called shallow parsing and it's basically the identification of parts of speech and\n",
    "          short phrases (like noun phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_new():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)#tagging part of speech to each word\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exeption as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Chinking\n",
    "        . removal from chunks (for now say it this way)\n",
    "        . chunk all this except this kinda\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_new_new():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)#tagging part of speech to each word\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT>+{\"\"\" #keep all(previous line) except this\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exeption as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content_new_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Named Entity Recognition\n",
    "        .The task of identifying proper names of people, organizations, locations, or other entities is a subtask \n",
    "         of information extraction from natural language documents\n",
    "        .the extraction of named entities, e.g. persons, organizations, or locations from corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNE Type and Examples\\nORGANIZATION - Georgia-Pacific Corp., WHO\\nPERSON - Eddy Bonte, President Obama\\nLOCATION - Murray River, Mount Everest\\nDATE - June, 2008-06-29\\nTIME - two fifty a m, 1:30 p.m.\\nMONEY - 175 million Canadian Dollars, GBP 10.40\\nPERCENT - twenty pct, 18.75 %\\nFACILITY - Washington Monument, Stonehenge\\nGPE - South East Asia, Midlothian\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NE Type and Examples\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_ner():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)#tagging part of speech to each word\n",
    "            \n",
    "            namedEntity = nltk.ne_chunk(tagged)\n",
    "            \n",
    "            print(namedEntity)\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exeption as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content_ner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Lemmatizing\n",
    "        . Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms\n",
    "          of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nucleus\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"nuclei\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos='a'))#part of speech adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Playing around a little bit with corpus , check for data file in AppData in your windows machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_sent = sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses:  Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.', '1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep.', 'And the Spirit of God moved upon the face of the\\nwaters.', '1:3 And God said, Let there be light: and there was light.', '1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.']\n"
     ]
    }
   ],
   "source": [
    "print(tok_sent[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Wordnet\n",
    "         . a large corpus very helpful see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\") #sets of cognitive synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plan.n.01'),\n",
       " Synset('program.n.02'),\n",
       " Synset('broadcast.n.02'),\n",
       " Synset('platform.n.02'),\n",
       " Synset('program.n.05'),\n",
       " Synset('course_of_study.n.01'),\n",
       " Synset('program.n.07'),\n",
       " Synset('program.n.08'),\n",
       " Synset('program.v.01'),\n",
       " Synset('program.v.02')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('plan.n.01')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('plan.n.01.plan'),\n",
       " Lemma('plan.n.01.program'),\n",
       " Lemma('plan.n.01.programme')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'broadcast'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[2].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('evil.n.03.evil')]\n",
      "[Lemma('bad.n.01.bad')]\n",
      "[Lemma('bad.a.01.bad')]\n",
      "[Lemma('evil.a.01.evil')]\n",
      "[Lemma('ill.r.01.ill')]\n"
     ]
    }
   ],
   "source": [
    "#for antonyms you have to use lemmas\n",
    "#here evil.n.03 means evil noun no.3\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    if syn.lemmas()[0].antonyms():\n",
    "        print(syn.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a radio or television show'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[2].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['did you see his program last night?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[2].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'respectable', 'secure', 'honorable', 'skillful', 'undecomposed', 'ripe', 'unspoiled', 'unspoilt', 'right', 'goodness', 'good', 'honest', 'upright', 'soundly', 'skilful', 'adept', 'sound', 'thoroughly', 'full', 'well', 'salutary', 'in_force', 'beneficial', 'effective', 'near', 'trade_good', 'proficient', 'just', 'estimable', 'safe', 'commodity', 'serious', 'dependable', 'in_effect', 'practiced', 'expert', 'dear'}\n",
      "-----------\n",
      "{'evil', 'bad', 'ill', 'badness', 'evilness'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print('-----------')\n",
    "print(set(antonyms))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    semantic similarity\n",
    "        . Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance \n",
    "          between them is based on the likeness of their meaning or semantic content as opposed to similarity \n",
    "          which can be estimated regarding their syntactical representation (e.g. their string format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "w1.wup_similarity(w2)\n",
    "#here we can see 90% similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"dog.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"truck.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "w1.wup_similarity(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
